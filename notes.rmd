---
title: 'Notes from *Introduction to Empirical Bayes* by David Robinson'
author: 'Péter A. Lukács'
output:
    html_document:
        fig_width: 9
        fig_height: 6
        fig_align: center
---

```{r 'load-packages', echo=FALSE, warning=FALSE}
source('global.R')

theme_set(theme_light())
theme_update(axis.title = element_text(size = 14),
             axis.text = element_text(size = 14),
             title = element_text(size = 16))
```

# I Empirical Bayes

## 2 The beta distribution

  + The beta distribution is a probability distribution with two parameters `alpha` and `beta`, constrained to between 0 and 1.

```{r 'beta-distro-plot', echo=FALSE}
beta_distro_params <- data.frame(a = c(1, 3, 20, 50), b = c(2, 3, 20, 10))
beta_distro_dt <- as.data.table(merge(beta_distro_params, x = seq(0, 1, 0.001), all = TRUE)) %>%
    .[, beta_density_value := dbeta(x, a, b), by = .(x, a, b)] %>%
    .[, Parameters := sprintf('a: %d, b: %d', a, b)]
ggplot(beta_distro_dt, aes(x, beta_density_value, color = Parameters)) +
    geom_line() +
    scale_y_continuous(name = 'Density of beta') +
    ggtitle('The beta distribution')
```

  + **Batting average** is an important baseball statistics which is calculated as the number of **hits (H)** divided by the number of **at-bats (AB)**. A player's batting average is always between 0 and 1 and can be represented with the beta distribution.

    \[Batting\ Average = \frac{H}{AB}\]

  + If a player starts out with a single or a strike out we won't predict that his BA will be 1 or 0. This is because we've seen in the history that players tend to have a BA between .270 and .300, thus we have *prior expectations*.

    > The number of hits a player gets out of his at-bats is an example of a **binomial distribution**, which models a count of successes out of a total. Since it's a binomial, the best way to represent the prior expectations is with the beta distribution.

    > This is the Bayesian philosophy in a nutshell: we start with a prior distribution, see some evidence, then update to a **posterior** distribution.

  + Update the beta distribution this way:
    \[Beta(\alpha_0 + hits,\ \beta_0 + misses)\]

  + Below is an example of a player: first, our prior expectation about him is that he will have around .27 BA ($Beta(81, 219)$). After this, he hits the ball once out of one, so we update his posterior distribution to $Beta(82, 219)$, a tiny change. Later he will be up to bat 300 times and hits the ball a 100 times out of that, updating his beta to $Beta(181, 419)$.

```{r 'beta-distro-update-plot', echo=FALSE}
beta_distro_params <- data.frame(a = c(81, 82, 181), b = c(219, 219, 419))
beta_distro_dt <- as.data.table(merge(beta_distro_params, x = seq(0, .5, 0.001), all = TRUE)) %>%
    .[, beta_density_value := dbeta(x, a, b), by = .(x, a, b)] %>%
    .[, Parameters := sprintf('a: %d, b: %d', a, b)]
ggplot(beta_distro_dt, aes(x, beta_density_value, color = Parameters)) +
    geom_line() +
    scale_color_manual(values = c('dodgerblue', 'green3', 'violetred')) +
    scale_y_continuous(name = 'Density of beta') +
    ggtitle('The updated beta distribution') +
    labs(x = 'Batting Average') +
    theme_light()
```

  + **Posterior mean** is the expected value of the resulting beta distribution which we can use as our new estimate.
    \[E(Beta(\alpha, \beta)) = \frac{\alpha}{\alpha + \beta}\]
  + With the above example you can see that our posterior expectation is higher than our prior but lower than our expectation based on the actual inspected hits and misses:
    \[\frac{81}{81 + 219} < \frac{182}{182 + 419} < \frac{100}{100 + 200}\]

  + Imagine that our objective is to assess a player who we've seen hit 100/300. We can't take every person with the exact stats and see how they did historically but we can do a simulation:

```{r 'simulate-100/300-player'}
num_trials <- 10e6

# Take 10mm players whose betting average produces a Beta(81, 219)
# Then, simulate how well they'd do if they tried to hit the ball 300 times
simulations <- data_frame(
    true_average = rbeta(num_trials, 81, 219),
    hits = rbinom(num_trials, 300, true_average)
)

simulations

# Now filter for those who hit exactly 100/300 and then plot their true average distribution
hit_100 <- simulations %>%
    filter(hits == 100)
```

```{r 'hit-100-plot', echo=FALSE, warning=FALSE}
dens <- function(x) dbeta(x, 81 + 100, 219 + 200)

ggplot(hit_100, aes(true_average)) +
    geom_histogram(aes(y = ..density..)) +
    stat_function(color = 'red', fun = dens) +
    labs(x = 'Batting average of players who got 100 H / 300 AB')
```

  + The median of the histogram (0.3) is our posterior estimate: we believe that a player with history of 100/300 is likely to have a BA of 0.3.
  + The histogram of the 100/300 batters and the well-fitted Beta density plot (red) confirms the math about the conjugate prior: you can calculate the posterior estimate without running a simulation.
  + See below cases where players hit 60, 80 or 100 times based on the simulation

```{r 'hit-60-80-100-plot', echo=FALSE, warning=FALSE}
simulations %>%
    filter(hits %in% c(60, 80, 100)) %>%
    ggplot(aes(true_average, color = factor(hits))) +
        geom_density() +
        labs(x = 'Batting average of players who got 60, 80, 100 H / 300 AB',
                 color = 'H')
```

## 3 Empirical Bayes estimation

  + This method will fit a beta distribution on all observations which is then used to improve each individually. This way we won't need to have prior expectations

#### 3.1 Setup: the Lahman baseball dataset

  + From now on we'll be working on real data from the *Lahman baseball dataset*

```{r 'lahmna-dataset', warning=FALSE}
# Filter out pitchers (b/c they are unusually weak batters)
career <- Batting %>%
    filter(AB > 0) %>%
    anti_join(Pitching, by = 'playerID') %>%
    group_by(playerID)  %>%
    summarize(H = sum(H), AB = sum(AB)) %>%
    mutate(average = H / AB)

# Include names
career  <- Master %>%
    tibble::as_tibble() %>%
    select(playerID, nameFirst, nameLast) %>%
    unite(name, nameFirst, nameLast, sep = ' ') %>%
    inner_join(career, by = 'playerID')

career
```

The following tables show you that simply using the average column won't get us the best or worst players

```{r 'best-worst-average', echo=FALSE, warning=FALSE}
career %>% top_n(5, average)
career %>% top_n(-5, average)
```

#### 3.2 Step 1: Estimate a prior from all the data

  + It's sometimes not appropriate to use the data we analyze to calculate the priors but with the amount of data we have currently it's fine, since the estimate won't depend much on any individual.
  + The below histogram shows that a beta distribution is a pretty appropriate choice for our data

```{r 'histogram-of-BAs', echo=FALSE, warning=FALSE}
career %>%
    filter(AB > 500) %>%
    ggplot(aes(average)) +
        geom_histogram(bins = 50)
```

  + Now we would need to fit the following model to find the $\alpha_0$ and $\beta_0$ *hyper-parameters*:

  \[ X \sim Beta(\alpha_0, \beta_0) \]

```{r 'maximum-likelihood'}
career_filtered <- career %>%
    filter(AB > 500)

log_likelihood <- function(alpha, beta) {
    x <- career_filtered$H
    total <- career_filtered$AB
    -sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}

max_likelihood_estimation <- mle(log_likelihood, start = list(alpha = 1, beta = 10), method = 'L-BFGS-B',
                                 lower = c(0.0001, .1))

ab <- coef(max_likelihood_estimation)

alpha0 <- ab[1]
beta0 <- ab[2]

dens <- function(x) dbeta(x, alpha0, beta0)

ggplot(career_filtered, aes(average)) +
    geom_histogram(aes(y = ..density..), bins = 50) +
    stat_function(color = 'red', fun = dens, size = 2) +
    labs(title = 'Histogram of BAs and the maximum likelihood fitted beta density distribution',
         x = 'BA',
         y = 'density')
```

  + The maximum likelihood model came up with $\alpha_0 =$ `r round(alpha0, 2)` and $\beta_0 =$ `r round(beta0, 2)`. We can see from the above plot that it fits the actual data pretty well.

#### 3.3 Step 2: Use that distribution as a prior for each individual estimate

  + Now we can update our data with our priors like so:

\[\frac{H + \alpha_0}{AB + \alpha_0 + \beta_0}\]

```{r 'empirical-bayes-estimate'}
career_eb <- career %>%
    mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0))
```

#### 3.4 Results

  + And we're able to ask who is the best/worst player:

```{r 'best-wors-emipircal-bayes-estimate', echo = FALSE}
career_eb %>% top_n(5, eb_estimate)
career_eb %>% top_n(-5, eb_estimate)
```

  + See below how empirical Bayes changed our estimate of batting averages:

```{r 'empirical-vs-bayesian-batting-averages', echo = FALSE}
ggplot(career_eb, aes(average, eb_estimate, color = AB)) +
    geom_point() +
    geom_hline(yintercept = alpha0 / (alpha0 + beta0), color = 'red', lty = 2) +
    geom_abline(color = 'red') +
    scale_colour_gradient(trans = 'log', breaks = 10 ^ (1:5)) +
    xlab('Batting average') +
    ylab('Empirical Bayes batting average')
```

  + The horizontal dashed lines shows $\frac{\alpha_0}{\alpha_0 + \beta_0}$. That would be our Bayesian estimate if someone would have $AB = H = 0$. The red diagonal line shows $x = y$. Points near that line are the brightest, because the more evidence we have, the less we distort (update) the average with the priors.

  > This process is often called **shrinkage**: the process of moving all our estimates towards the average. Or in other words: *Extraordinary outliers require extraordinary evidence.*

## 4. Credible Intervals

  > Empirical Bayes gives us a reliable estimate but sometimes we want to know more than just our "best guess", and instead wish to know how much uncertainty is present in our point estimate

  + The problem with binomial proportion confidence interval is that is doesn't use prior knowledge. As a result for betters with short history it would give a huge confidence range

#### 4.1 Setup [intentionally left out]

#### 4.2 Posterior distribution

  + Apart from calculating point estimates from priors and evidence from players we can also calculate an updated personal Beta distribution for each player. What we are looking for is $\alpha_1 = \alpha_0 + H$ and $\beta_1 = \beta_0 + AB - H$

```{r 'posterior-distributions'}
career_eb <- career_eb %>%
    mutate(alpha1 = alpha0 + H,
           beta1 = beta0 + AB - H)
```

```{r 'posterior-distributions-plot', echo = FALSE}

yankee_1998 <- c('brosisc01', 'jeterde01', 'knoblch01', 'martiti02',
                 'posadjo01', 'strawda01', 'willibe02', 'aaronha01')

yankee_1998_career <- career_eb %>%
    filter(playerID %in% yankee_1998)

yankee_beta <- yankee_1998_career %>%
    crossing(x = seq(.18, .33, .0002)) %>%
    ungroup() %>%
    mutate(density = dbeta(x, alpha1, beta1))

ggplot(yankee_beta, aes(x, density, color = name)) +
    geom_line() +
    stat_function(fun = function(x) dbeta(x, alpha0, beta0),
                lty = 2, color = "black") +
    labs(x = "Batting average",
         color = "Player")
```

#### 4.3 Credible intervals

  + These personal distributions are hard to interpret. We rather have information on how much (e.g. 95%) of the posterior distribution lies within a particular region. A credible interval for Derek Jeter is shown below, while 95% credible intervals for the other players under that.

```{r 'jeter-credible-interval'}
jeter <- yankee_beta %>%
    filter(name == "Derek Jeter")

jeter_low <- qbeta(.025, jeter$alpha1[1], jeter$beta1[1])
jeter_high <- qbeta(.975, jeter$alpha1[1], jeter$beta1[1])

jeter %>%
    ggplot(aes(x, density)) +
        geom_line() +
        geom_ribbon(aes(ymin = 0, ymax = density), data = setDT(jeter)[x > jeter_low & x < jeter_high],
                    alpha = .25, fill = "red") +
        stat_function(fun = function(x) dbeta(x, alpha0, beta0),
                    lty = 2, color = "black") +
        geom_errorbarh(aes(xmin = jeter_low, xmax = jeter_high, y = 0), height = 3.5, color = "red") +
        xlim(.18, .34) +
        labs(title = 'The posterior beta distribution for Derek Jeter',
             subtitle = '3465 H / 11195 AB, with the 95% credible interval')
```


```{r 'add-credible-intervals'}
yankee_1998_career <- yankee_1998_career %>%
    mutate(low  = qbeta(.025, alpha1, beta1),
           high = qbeta(.975, alpha1, beta1))
```

```{r 'credible-interval-table', echo = FALSE}
yankee_1998_career %>%
    dplyr::select(-playerID, -alpha1, -beta1, -eb_estimate) %>%
    knitr::kable()
```

```{r 'credible-interval-for-the-rest-of-the-players'}
yankee_1998_career %>%
    mutate(name = reorder(name, eb_estimate)) %>%
    ggplot(aes(eb_estimate, name)) +
        geom_point() +
        geom_errorbarh(aes(xmin = low, xmax = high)) +
        geom_vline(xintercept = alpha0 / (alpha0 + beta0), color = "red", lty = 2) +
        xlab("Estimated batting average (w/ 95% interval)") +
        ylab("Player")
```

#### 4.4 Credible intervals (CrI) and confidence intervals (CoI)

  + There is a philosophical and a practical difference between the above two:
      * Philosophical: The frequentist method (CoI) assumes that the parameter that we are estimating is an exact number and that the CoI will include that number X% of the times. The Bayesian method assumes that the parameter is picked from a distribution and the CrI describes this distribution.
      * Practical: While the frequentist method doesn't use the information of other observations, the Bayesian method does so by calculating the priors.
  + Frequentist and Bayesian intervals and estimates are becoming identical once we have many enough observations (see below)

```{r 'credible-interval-vs-confidence-interval-plot', echo = FALSE}
career_eb <- career_eb %>%
    mutate(low = qbeta(.025, alpha1, beta1),
           high = qbeta(.975, alpha1, beta1))

set.seed(2018)

some <- career_eb %>%
    sample_n(20) %>%
    mutate(name = paste0(name, " (", H, "/", AB, ")"))

frequentist <- some %>%
    group_by(playerID, name, AB) %>%
    do(broom::tidy(binom.test(.$H, .$AB))) %>%
    ungroup() %>%
    dplyr::select(playerID, name, estimate, low = conf.low, high = conf.high) %>%
    mutate(method = "Confidence")

bayesian <- some %>%
      dplyr::select(playerID, name, AB, estimate = eb_estimate,
                    low = low, high = high) %>%
      mutate(method = "Credible")

combined <- bind_rows(frequentist, bayesian)

combined %>%
    mutate(name = reorder(name, -AB, na.rm = TRUE)) %>%
    ggplot(aes(estimate, name, color = method, group = method)) +
        geom_point() +
        geom_errorbarh(aes(xmin = low, xmax = high)) +
        geom_vline(xintercept = alpha0 / (alpha0 + beta0), color = "red", lty = 2) +
        xlab("Estimated batting average") +
        ylab("Player") +
    labs(color = "")
```

# II Hypothesis testing

## 5. Hypothesis testing and FDR

#### 5.1 Setup [intentionally left out]

#### 5.2 Posterior Error Probabilities (PEP)

  + Imagine a situation where we'd like to make a Hall of Fame for players whose "true probability" of hitting is above 0.3.
  + Take e.g. Hank Aaron: his batting average is 0.3050 and his shrunken posterior estimate is 0.3037. We can thus suspect that his true probability of hitting is higher than 0.3 but we are not certain. We can take a look at his posterior beta distribution

```{r 'hank-aaron-PEP-plot', echo = FALSE, warning = FALSE}
aaron <- yankee_beta %>%
    filter(name == "Hank Aaron")

aaron %>%
    ggplot(aes(x, density)) +
        geom_line() +
        geom_ribbon(aes(ymin = 0, ymax = density), data = setDT(aaron)[x <= .300],
                    alpha = .25, fill = "red") +
        geom_vline(xintercept = .300, color = "red", lty = 2) +
        xlim(.28, .32) +
        labs(title = 'The posterior beta distribution for Derek Jeter',
             subtitle = 'PEP highlighted with red - for considering inclusion into the Hall of Fame (BA > 0.3)',
             x = 'Betting average',
             y = 'density')
```

  + See, that there is a non-zero probability that his true probability is less than 0.3 (shaded). We can calculate the size of this area with the cumulative distribution function (CDF). In R: `pbeta`
  + This probability is called the **Posterior Error Probability**

```{r 'hank-aaron-PEP-probability'}
    hanka_hyper_params <- career_eb  %>%
        filter(name == 'Hank Aaron') %>%
        .[, c('alpha1', 'beta1')]

    pbeta(.3, hanka_hyper_params[['alpha1']], hanka_hyper_params[['beta1']]) %>% scales::percent()
```

  + We can also calculate the PEP for each player and see its distribution below

```{r 'all-players-PEP-probability'}
    career_eb <- career_eb  %>%
        mutate(PEP = pbeta(0.3, alpha1, beta1))

    ggplot(career_eb, aes(PEP)) + geom_histogram(bins = 50)
```

  + See that most players clearly don't belong to the Hall of Fame, a few do without any doubt and then there are some in between.

  + PEP is of course closely related to the Bayesian "shrunken" posterior BA estimate. Players with higher estimate are more likely to belong to the Hall. Also, see that players with less evidence have a higher chance of getting into the Hall than those with more evidence but same BA. This is because we are yet uncertain about their "true" BA due to their low number of at-bats (evidence). See below the relationship:

```{r 'PEP-vs-bayesian-posterior-batting-average', echo = FALSE}
ggplot(career_eb, aes(eb_estimate, PEP, color = AB)) +
    geom_point() +
    geom_vline(xintercept = .3, color = 'red', lty = 2) +
    scale_colour_gradient(trans = 'log', breaks = 10 ^ (1:5)) +
    xlab('Bayesian "shrunken" posterior BA estimate')
```

#### 5.3 False Discovery Rate (FDR)

  + We need to find a threshold for inclusion. This is arbitrary. One common solution in statistics is: "let's try to include as many players as possible, while ensuring that no more than 5% of the Hall of Fames was mistakenly included." Or in other words: "If you are in the Hall of Fames, the probability you belong there is at least 95%".

  + We calculate the 5% FDR by ordering the players by PEP (lowest to highest) and start adding up (cumulatively) their PEP. If we select the top 100 players we could see e.g. that their PEP adds up to 5.9, meaning that 5.9% of the players are estimated to be included by mistake, which is close to a 6% FDR. With a bit lower threshold we are able to find the 5% FDR.

```{r 'FDR-for-100-players'}
top_players <- career_eb %>%
    arrange(PEP) %>%
    head(100)

scales::percent(
    sum(top_players$PEP) / nrow(top_players)
)
```

  + Note, that simply calculating the cumulative mean achieves the same results

#### 5.4 Q-values

  + We can start adding each player one by one and calculate the FDR at each new inclusion. This way we can calculate the cumulative mean PEP at each threshold, aka. the **q-value**.

```{r 'cummean-PEP-at-each-threshold'}
career_eb <- career_eb %>%
    arrange(PEP) %>%
    mutate(qvalue = cummean(PEP))

hall_of_fame <- career_eb %>%
    filter(qvalue < .05)

nrow(hall_of_fame)
```

```{r 'q-value-plot', echo = FALSE}
career_eb %>%
    filter(qvalue < .3) %>%
    ggplot(aes(qvalue, rank(PEP))) +
        geom_line() +
        geom_vline(xintercept = 0.05, color = "red", lty = 2) +
        scale_x_continuous(breaks = seq(0, 1, 0.05), labels = scales::percent_format()) +
        ggrepel::geom_label_repel(data = data.table(label = nrow(hall_of_fame), x = 0.05, y = nrow(hall_of_fame)), mapping = aes(x, y, label = label), nudge_y = 10, nudge_x = -0.01) +
        xlab("q-value threshold") +
        ylab("Number of players included at this threshold") +
        labs(title = 'Number of players included in the Hall of Fame at each FDR',
             subtitle = '5% FDR shown with the red line')
```

## 6. Bayesian A/B testing

#### 6.1 Setup [intentionally left out]

#### 6.2 Comparing posterior distributions

  + How to tell if Mike Piazza is better than Hank Aaron? His career batting average is slightly better but how confident are we that this isn't just chance?

```{r 'aaron-and-piazza', echo = FALSE, warning = FALSE}
aaron <- career_eb %>% filter(name == "Hank Aaron")
piazza <- career_eb %>% filter(name == "Mike Piazza")
two_players <- bind_rows(aaron, piazza)

two_players_beta <- two_players %>%
    crossing(x = seq(.18, .33, .0002)) %>%
    ungroup() %>%
    mutate(density = dbeta(x, alpha1, beta1))

ggplot(two_players_beta, aes(x, density, color = name)) +
    geom_line() +
    xlim(c(0.28, NA)) +
    labs(color = "", x = 'Bayesian BA')
```

  + There are 4 ways that can quantify which player is better. I'll only include the first one in this note:
    + Simulation: No math involved (+)
    + Integration: Difficult when there are more than two dimensions (-)
    + Closed-form solution: Can be hard and not always a solution (-); Exact (+)
    + Closed-form approximation: Fast (+); Beta can only be approximated with the Normal curve when hyper-parameters are high (-)

##### 6.2.1 Simulation of posterior draws

  + Use each player's $\alpha_1$ and $\beta_1$ parameters, draw a million items from each distribution and compare the results

```{r 'A/B simulation'}
    piazza_simulation <- rbeta(1e6, piazza$alpha1, piazza$beta1)
    aaron_simulation <- rbeta(1e6, aaron$alpha1, aaron$beta1)

    sim <- mean(piazza_simulation > aaron_simulation)
    sim %>% scales::percent()
```

  + This tells us that we are 59.4% certain that Piazza is better than Aaron. This answer is usually good enough depending on the need for precision and computational efficiency.

# III Extending the model

## 7 Beta binomial regression

  + **problem**: so far we dismissed the intuition that a player with higher AB probably gets played more because he is good
  + This could be incorporated into the priors so that now every player would have a personal prior: we can fit a linear model on the data where the BA is linearly dependent on the ABs.
  + In this model the variance of the prior Beta distribution is not effected
  + In **R** the `gamlss` library can be used to fit a Beta binomial regression
